{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6884,
     "status": "ok",
     "timestamp": 1733758096293,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "ajoV6C_z9g8h",
    "outputId": "f8522687-48d1-4dc8-e46b-9acc2858b1f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymrio in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (0.5.4)\n",
      "Requirement already satisfied: pandas>=1.5 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pymrio) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=11.0 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pymrio) (18.1.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pymrio) (2.2.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pymrio) (3.9.3)\n",
      "Requirement already satisfied: requests>=2.18 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pymrio) (2.32.3)\n",
      "Requirement already satisfied: xlrd>1.1.0 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pymrio) (2.0.1)\n",
      "Requirement already satisfied: openpyxl<3.1.1,>=3.0.6 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pymrio) (3.1.0)\n",
      "Requirement already satisfied: docutils>=0.14 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pymrio) (0.21.2)\n",
      "Requirement already satisfied: faker>=18.4.0 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pymrio) (33.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from faker>=18.4.0->pymrio) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from faker>=18.4.0->pymrio) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib>=2.0.0->pymrio) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib>=2.0.0->pymrio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib>=2.0.0->pymrio) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib>=2.0.0->pymrio) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib>=2.0.0->pymrio) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib>=2.0.0->pymrio) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib>=2.0.0->pymrio) (3.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from openpyxl<3.1.1,>=3.0.6->pymrio) (2.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pandas>=1.5->pymrio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pandas>=1.5->pymrio) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from requests>=2.18->pymrio) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from requests>=2.18->pymrio) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from requests>=2.18->pymrio) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from requests>=2.18->pymrio) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from python-dateutil>=2.4->faker>=18.4.0->pymrio) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from seaborn) (2.2.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from seaborn) (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/michelepalma/Downloads/ExpPov-CARBONEM/.venv/lib64/python3.13/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymrio\n",
    "import pymrio\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1733758101597,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "rtFQsXV9ClTC"
   },
   "outputs": [],
   "source": [
    "#Data cleaning process for coicop codes (hbs_conversion_weight_table)\n",
    "def clean_up(text):\n",
    "    cleaned = text.translate(str.maketrans('', '', string.punctuation+string.digits+'\\n')).lower().strip()\n",
    "    return cleaned\n",
    "\n",
    "def clean_coicop_label(label):\n",
    "    return re.sub(r'\\(.*?\\)', '', label).strip()\n",
    "\n",
    "# Function to process COICOP codes (make them HBS-COMPATIBLE)\n",
    "def process_code(code):\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    code = code.lstrip('c')\n",
    "    code = code.replace('.', '')\n",
    "    return f'EUR_HE{code}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0c2fk4sGJD4k"
   },
   "outputs": [],
   "source": [
    "def get_region_hbs_fts(X_i, weight_map):\n",
    "  \"\"\"Given a region X_i (a column from X) it computes the corresponding region footprints for hbs EUR_HE only  \n",
    "\n",
    "    X the whole matrix \n",
    "    returns a table in the form: category| footprint|\n",
    "                                 EUR_HExx| value\n",
    "  \"\"\"\n",
    "  data = pd.DataFrame(columns=['category','footprint'])\n",
    "  #Determine HBS Categories env impacts starting from EXIOBASE env impacts  (following weight_map)\n",
    "  regions = X_i.index.levels[0]\n",
    "  categories = weight_map.index\n",
    "\n",
    "  for hbscat in categories:\n",
    "    cat_footprint=0\n",
    "    resp_cat = weight_map.loc[hbscat]\n",
    "    resp_cat = resp_cat[resp_cat!=0].index.tolist()\n",
    "    for exiocat in resp_cat:\n",
    "      rows_sum=X_i.loc[X_i.index.get_level_values('sector') == exiocat].sum()\n",
    "      cat_footprint += (weight_map.at[hbscat, exiocat]*rows_sum)\n",
    "\n",
    "    new_row = pd.DataFrame([[hbscat, cat_footprint]], columns=['category', 'footprint'])\n",
    "    data = pd.concat([data, new_row], ignore_index=True)\n",
    "  \n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1733758103420,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "3AVEcC4l-Ix6"
   },
   "outputs": [],
   "source": [
    "#HBS Env factors\n",
    "def get_hbs_env(country_hbs,country_env):\n",
    "  \"\"\"\n",
    "  Returns the hbs file where only category expenditures are replaced with corresponding footprint values\n",
    "\n",
    "  country_hbs:\n",
    "              is the hbs_hh file for the given country (and given year) e.g. IT_HBS_2010\n",
    "  country_env:\n",
    "              is a footprint hashmap: where keys are strings of  hbs EUR categories, and values contain unitary footprint associated to the category. (these footprints are specific to the given country and year)\n",
    "              e.g. 'EUR_HE011' = 2\n",
    "                   'EUR_HJ01' =  3\n",
    "                   for IT 2010\n",
    "\n",
    "  NB:The country_env MUST contain all hbs expenditure categories for which env factors are computed, variables not affected should be omitted (please refer to HBS UserManual)\n",
    "     categories different from EUR_HE or EUR_HJ should always be omitted.\n",
    "     if the footprint estimate for any expenditure category is not available then its value in country_env must be at 0\n",
    "  \"\"\"\n",
    "\n",
    "  #create extended footprints vector (country specific) with same format as hbs file headers\n",
    "  ex_ft = country_hbs.iloc[0:1].copy() # Create a copy of the original DataFrame\n",
    "  ex_ft.loc[:] = 1\n",
    "\n",
    "\n",
    "  #for each hbs EUR cat in country_env set the value in ex_ft\n",
    "  for EUR_cat in country_env.keys():\n",
    "    if EUR_cat in ex_ft.columns: #safety check, avoids adding non-existing cols\n",
    "      ex_ft[EUR_cat] = country_env[EUR_cat]\n",
    "      country_hbs[EUR_cat]=country_hbs[EUR_cat].fillna(0) #avoids NaN values in later operations\n",
    "\n",
    "\n",
    "  #perform element wise broadcasted multiplication\n",
    "  headers = country_hbs.columns\n",
    "  A = np.array(country_hbs)\n",
    "  x = np.array(ex_ft)\n",
    "\n",
    "  for EUR_cat in country_env.keys():\n",
    "      headers = [header.lower() if header == EUR_cat else header for header in headers]\n",
    "\n",
    "  # Reshape x to make it compatible for broadcasting along the columns of A\n",
    "  # Perform element-wise multiplication using broadcasting\n",
    "  B = A * x  # Each column of A is multiplied by the corresponding element in x\n",
    "  #re-attach hbs headers\n",
    "  B_df = pd.DataFrame(B, columns=headers)\n",
    "  return B_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "kpWWcN5XKA_x"
   },
   "outputs": [],
   "source": [
    "class Aggregator():\n",
    "\n",
    "    def __init__(self, level_map):\n",
    "        \"\"\"\n",
    "        A simple aggregator\n",
    "        takes an hash map, where key is the level and value is a set of labels categories for the level\n",
    "        where each element is a set of labels categories for the specific level\n",
    "        lev_labels_list first element is a set of categories for the first level, ..., lev_labels_list[i] must contain set categories for level[i]\n",
    "\n",
    "\n",
    "        the order is dictated by the fact that categories at level [i+1] are subcategories of categories at level[i]\n",
    "        \"\"\"\n",
    "\n",
    "        self.number_levels = len(level_map)\n",
    "        self.level_map = level_map\n",
    "\n",
    "\n",
    "    def get_aggregates(self, df):\n",
    "        \"\"\"Pass a dataframe that contains as column headers the set given by the union of label sets present in each level of the level_map\n",
    "          the values of such df are the ones that needs to be aggregated column-wise\n",
    "\n",
    "        \"\"\"\n",
    "        aggregates = df.copy()\n",
    "        #from the bottom level to upper level (0) (it skips the lowest level as no aggregation is needed)\n",
    "        for level in range(len(self.level_map)-1, 0,-1):\n",
    "            current_lev_cats = self.level_map[level]\n",
    "            for cat in current_lev_cats:\n",
    "                subcat_to_sum = self.level_map[level+1]\n",
    "                pattern = rf\"^{re.escape(cat)}\"\n",
    "                subcat_to_sum = set([eur_cat for eur_cat in subcat_to_sum if re.match(pattern, eur_cat)])\n",
    "                #aggregate level-wise (bottom-up) for EUR categories\n",
    "                #recall now modified cat are in lower_case\n",
    "\n",
    "                Sum = pd.DataFrame({\n",
    "                    'sum': [0] * len(df)\n",
    "                })\n",
    "\n",
    "                for subcat in subcat_to_sum:\n",
    "                  Sum['sum'] +=  df[subcat.lower()]\n",
    "\n",
    "                if (aggregates[cat.lower()] == 0).all():\n",
    "                    aggregates[cat.lower()] = Sum['sum']\n",
    "                elif (aggregates[cat.lower()] < Sum['sum']).any():\n",
    "                    print('Please check correctness of your footprint estimates! for '+cat+' category and '+str(subcat_to_sum))\n",
    "\n",
    "\n",
    "        return aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1733758099452,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "ihIk8R_O9w1P"
   },
   "outputs": [],
   "source": [
    "#--ARGS--\n",
    "REGIONS = [\"BE\",\"BG\",\"CY\",\"CZ\",\"DE\",\"DK\",\"EE\",\"EL\",\"ES\",\"FI\",\"FR\",\"HR\",\"HU\",\"IE\",\"IT\"]\n",
    "YEARS = [\"2010\"]\n",
    "ENV_FACTS = [\"GHG emissions (GWP100) | Problem oriented approach: baseline (CML, 2001) | GWP100 (IPCC, 2007)\"]\n",
    "on_Colab = False  #set this to true if working on Colab\n",
    "working_DIR=\"carb_em_project\"\n",
    "working_DIR =  os.path.join(os.getcwd(), working_DIR) \n",
    "\n",
    "if on_Colab:\n",
    "    from google.colab import drive\n",
    "    working_DIR=\"/content/drive/MyDrive/carb_em_project\"\n",
    "\n",
    "weight_table_PATH = working_DIR\n",
    "#load the coicop - exiobase conversion weights map (aka weight_table)\n",
    "weights = pd.read_excel(os.path.join(weight_table_PATH,'COICOP_EU_ini.xlsx') ,sheet_name='bridge_ini', header=1)\n",
    "weights = weights.iloc[:63, 2:] #exclude false row/cols\n",
    "weights = weights.rename(columns={'Unnamed: 2': 'COICOP'})\n",
    "#we match coicop labels from weight_table with hbs codes\n",
    "hash_labels = pd.read_excel(os.path.join(weight_table_PATH, 'COICOP_EXIOBASE.xlsx') ,sheet_name='COICOP_to_EXIOBASEprod', header=None)\n",
    "# clean the coicop labels and produce a map that reports the actual HBS code for the given COICOP category\n",
    "hash_labels[1] = hash_labels[1].apply(process_code)\n",
    "hash_labels[2] = hash_labels[2].apply(clean_coicop_label)\n",
    "hash_labels[2] = hash_labels[2].apply(clean_up)\n",
    "hash_map = hash_labels[[1, 2]].set_index(2).to_dict()[1]\n",
    "#Manually add missing labels - hbs code (Following HBS DOCUMENTATION)\n",
    "hash_map['nonalcoholic beverages']='EUR_HE012'\n",
    "hash_map['alcoholic beverages']='EUR_HE021'\n",
    "hash_map['clothing']='EUR_HE031'\n",
    "hash_map['footwear']='EUR_HE032'\n",
    "hash_map['actual rent']='EUR_HE041'\n",
    "hash_map['imputed rent']='EUR_HE042'\n",
    "hash_map['maintenance and repair of the dwelling']='EUR_HE043'\n",
    "hash_map['water supply and miscellaneous services reacting to the dwelling']='EUR_HE044'\n",
    "hash_map['household appliances']='EUR_HE053'\n",
    "hash_map['tools and equipment for house and garden']='EUR_HE055'\n",
    "hash_map['medical products appliances and equipment']='EUR_HE061'\n",
    "hash_map['outpatient services'] = 'EUR_HE062'\n",
    "hash_map['purchase of vehicles']='EUR_HE071'\n",
    "hash_map['communication'] = 'EUR_HE08'\n",
    "hash_map['audiovisual photographic and information processing equipment'] = 'EUR_HE091'\n",
    "hash_map['other major durables for recreation and culture']='EUR_HE092'\n",
    "hash_map['other recreational items and equipment gardens and pets'] = 'EUR_HE093'\n",
    "hash_map['recreational and cultural services']='EUR_HE094'\n",
    "hash_map['newspapers books and stationery'] = 'EUR_HE095'\n",
    "hash_map['preprimary and primary education secondary education postsecondary education tertiary education and education not defined by level'] = 'EUR_HE10'\n",
    "hash_map['catering services'] = 'EUR_HE111'\n",
    "hash_map['personal care']='EUR_HE121'\n",
    "hash_map['insurance']='EUR_HE125'\n",
    "hash_map = {key: value.strip() for key, value in hash_map.items()}\n",
    "display(hash_map)\n",
    "#replace coicop labels in original weight_table with actual HBS code using the previously created hash map\n",
    "weights['COICOP'] = weights['COICOP'].apply(clean_up)\n",
    "weights['COICOP'] = weights['COICOP'].replace(hash_map)\n",
    "weights.set_index('COICOP', inplace=True)\n",
    "weights = weights.iloc[:, :-1] #exclude last column (total)\n",
    "weights.columns = weights.columns.map(clean_up)\n",
    "display(weights)\n",
    "\n",
    "\n",
    "for year in YEARS:\n",
    "    working_DIR= os.path.join(working_DIR, year)\n",
    "    if not os.path.exists(working_DIR):\n",
    "        os.makedirs(working_DIR)\n",
    "\n",
    "    if on_Colab:  #actually this should be run always regardless of colab, but on low-end pc I use pre-computed tables\n",
    "        exiobase_Par=os.path.join(working_DIR,\"IOT_\"+year+\"_pxp.zip\")\n",
    "        exio_download = pymrio.download_exiobase3(storage_folder=working_DIR, system=\"pxp\", years=year)\n",
    "        exiobase3 = pymrio.parse_exiobase3(path=exiobase_Par)\n",
    "        exiobase3.calc_all()\n",
    "        _regions = exiobase3.Y.columns.get_level_values(0).unique()\n",
    "        Y_a = pd.DataFrame(index=exiobase3.Y.index)\n",
    "        for _region in _regions:\n",
    "            # Select all 7 columns for the region and aggregate them\n",
    "            region_columns = exiobase3.Y.loc[:, _region]\n",
    "            Y_a[_region] = region_columns.sum(axis=1)\n",
    "        \n",
    "\n",
    "        for env_fact in ENV_FACTS:\n",
    "            P=exiobase3.impacts.S.loc[env_fact]\n",
    "            P = P/1000000\n",
    "            X = exiobase3.L.dot(Y_a)\n",
    "            X = X.multiply(P, axis=0)  # element-wise multiplication\n",
    "            X.to_csv(os.path.join(working_DIR,\"X_\"+env_fact[:3]+\".csv\"), index=True,  header=True)\n",
    "\n",
    "    if not os.path.exists(working_DIR+\"/out\"):\n",
    "        os.makedirs(working_DIR+\"/out\")\n",
    "\n",
    "    for env_fact in ENV_FACTS:\n",
    "        X = pd.read_csv(os.path.join(working_DIR,\"X_\"+env_fact[:3]+\".csv\"),  index_col=[0, 1],)\n",
    "        _regions = X.columns.unique()\n",
    "        #clean Exiobase labels for X matrix\n",
    "        X.index = X.index.set_levels(\n",
    "                X.index.levels[1].map(clean_up),  \n",
    "                level=1                   \n",
    "        )\n",
    "\n",
    "        #compute for each region in hbs EUR_HE env footprints, and add them to ft table\n",
    "        footprints =  pd.DataFrame(columns=['region', 'category','footprint'])\n",
    "        for _region in _regions:\n",
    "            o=get_region_hbs_fts(X[_region],weights)\n",
    "            for index, row in o.iterrows():\n",
    "                new_row = {'region': _region, 'category': row['category'], 'footprint': row['footprint']}\n",
    "                footprints.loc[len(footprints)] = new_row\n",
    "\n",
    "        footprints.to_csv(os.path.join(working_DIR,\"footprints_\"+env_fact[:3]+\".csv\"), index=False)\n",
    "\n",
    "        for country_ticker in REGIONS:\n",
    "            #fixed a year, this runs region by region\n",
    "            hbs_hh= pd.read_excel(os.path.join(working_DIR,country_ticker+\"_HBS_hh.xlsx\"))\n",
    "            #building the footprint tables by adding only Expenditure categories vars of hbs file\n",
    "            EUR_HE_cats = [category for category in hbs_hh.columns if re.match(r\"^EUR_HE.*\", category)]  #HOME expens\n",
    "            EUR_HJ_cats = [category for category in hbs_hh.columns if re.match(r\"^EUR_HJ.*\", category)]  #ABROAD expens\n",
    "\n",
    "            ft = dict(zip(EUR_HE_cats, [0]*len(EUR_HE_cats)))   #from here one can notice that an envfactor of zero for a given expcategory  does not necessarily mean that its associated footprint is actually 0 (can be that we have no data on footprint of the category)\n",
    "            #ft will later contain the effective  region specific footprints for each category\n",
    "            #HBS structure presents EUR_HE cat in 4 levels (where first means the top-level categories):\n",
    "            level_map = {}\n",
    "            for i in range(5, 1, -1):\n",
    "                #determine level (i-1) EUR categories using RegEx and HBS structure guidelines\n",
    "                pattern = rf\"EUR_HE\\d{{{i}}}\\b\"\n",
    "                curr_level_cat = [category for category in (set(EUR_HE_cats)) if re.match(pattern, category)]\n",
    "                curr_level_cat = set(curr_level_cat)\n",
    "                level_map[i-1] = curr_level_cat\n",
    "\n",
    "            aggregator = Aggregator(level_map)\n",
    "            saving_path = os.path.join(working_DIR,\"out\")\n",
    "            \n",
    "            footprints = pd.read_csv(os.path.join(working_DIR,\"footprints_\"+env_fact[:3]+\".csv\"))\n",
    "            filtered_fact = footprints[footprints['region'] == country_ticker]\n",
    "\n",
    "            for index, row in filtered_fact.iterrows():\n",
    "                if row['category'] in ft.keys(): #avoids adding non-existing EUR_cat to ft\n",
    "                    ft[row['category']] = row['footprint']\n",
    "\n",
    "            result = get_hbs_env(hbs_hh,ft)\n",
    "            #result.to_csv(os.path.join(saving_path,country_ticker+'_'+year+'_'+\"ENVHBS_hh.csv\"), index=False)  before aggregation\n",
    "\n",
    "            #aggregate\n",
    "            result = aggregator.get_aggregates(result)\n",
    "            \n",
    "            #eur_he00 follows special aggregation process\n",
    "            Sum = pd.DataFrame({\n",
    "                    'sum': [0] * len(result)\n",
    "                })\n",
    "            level_cats = level_map[1] - set('EUR_HE00')\n",
    "            for cat in level_cats:\n",
    "                Sum['sum'] +=  result[cat.lower()]\n",
    "\n",
    "                if (result['eur_he00'] == 0).all():\n",
    "                    result['eur_he00'] = Sum['sum']\n",
    "                elif (result['eur_he00'] < Sum['sum']).any():\n",
    "                    print('Please check correctness of your footprint estimates! for EUR_HJ00 category and '+str(level_cats))\n",
    "            \n",
    "                #compute now eur_hjxx env fact\n",
    "            ft = dict(zip(EUR_HJ_cats, [0]*len(EUR_HJ_cats))) \n",
    "            result.to_csv(os.path.join(saving_path,country_ticker+'_'+year+'_'+env_fact[:3]+\"_ENVHBS_hh.csv\"), index=False)  \n",
    "            #divide each eur_he00 (first level, to which EUR_HJ are referred to) by EUR_HE00 (original hbs)\n",
    "            #use such values as env_footprint for eur_hj for current region\n",
    "\n",
    "            # Sum = pd.DataFrame({\n",
    "            #                 'sum': [0] * len(result)\n",
    "            #                     })\n",
    "            # level_cats = set(EUR_HJ_cats) - set(['EUR_HJ00', 'EUR_HJ90'])\n",
    "            # for cat in level_cats:\n",
    "            #     Sum['sum'] +=  result[cat.lower()]\n",
    "\n",
    "            #     if (result['eur_hj00'] == 0).all():\n",
    "            #         result['eur_hj00'] = Sum['sum']\n",
    "            #     elif (result['eur_hj00'] < Sum['sum']).any():\n",
    "            #         print('Please check correctness of your footprint estimates! for EUR_HJ00 category and '+str(level_cats))\n",
    "\n",
    "                        #produce_env_hbs(region,env_fact)  #args will be named:  country_ticker, year, env_fact\n",
    "                        #place again code agnostic blocks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pViiuTBs3Us2"
   },
   "source": [
    "### After Exiobase computation, use weight table to compute hbs env factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rag_Fa-V98R4"
   },
   "source": [
    "###Load HBS file structure and Compute Environmental HBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1733754767897,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "PP846oMA-S6_",
    "outputId": "660c3371-1c7b-4e6d-bb68-0d18221ad452"
   },
   "outputs": [],
   "source": [
    "#NOT IN USE\n",
    "#define hash_map for manual matching indirect home (HJ_xx) exploiting HE_xx\n",
    "i_hash_map = {}\n",
    "\n",
    "#Manually add missing labels - hbs code (Following HBS DOCUMENTATION)\n",
    "i_hash_map['EUR_HJ00'] = ''\n",
    "i_hash_map['EUR_HJ01']='EUR_HE01' #Food and non-alcoholic beverages\n",
    "i_hash_map['EUR_HJ02']='EUR_HE02' #Alcoholic beverages, tobacco and narcotics\n",
    "i_hash_map['EUR_HJ03']='' #Clothing and footwear\n",
    "i_hash_map['EUR_HJ04']='' #Housing, water, electricity, gas and other fuels\n",
    "i_hash_map['EUR_HJ05']='' #Furnishings, household equipment and routine household maintenance\n",
    "i_hash_map['EUR_HJ06']='' #Health\n",
    "i_hash_map['EUR_HJ07']='' #Transport\n",
    "i_hash_map['EUR_HJ08']='' #Communication\n",
    "i_hash_map['EUR_HJ09']='' #Recreation and culture\n",
    "i_hash_map['EUR_HJ10']='' #Education\n",
    "i_hash_map['EUR_HJ11']='' #Restaurants and hotels\n",
    "i_hash_map['EUR_HJ12']='' #Miscellaneous goods and services\n",
    "#instead of doing it manually\n",
    "for key in i_hash_map:\n",
    "    i_hash_map[key] = key.replace('J', 'E')\n",
    "i_hash_map['EUR_HJ90'] = '' #Consumption expenditure on travelling and holidays done abroad  (no MATCH for EUR_HExx)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMhsazwJ2NtRc0z/Zdsf+xJ",
   "mount_file_id": "11oKnHHLswjXyMLWgTeG1msFFTjxcWYiq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
