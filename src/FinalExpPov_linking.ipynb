{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6884,
     "status": "ok",
     "timestamp": 1733758096293,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "ajoV6C_z9g8h",
    "outputId": "f8522687-48d1-4dc8-e46b-9acc2858b1f2"
   },
   "outputs": [],
   "source": [
    "%pip install pymrio\n",
    "import pymrio\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1733758099452,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "ihIk8R_O9w1P"
   },
   "outputs": [],
   "source": [
    "#--ARGS--\n",
    "on_Colab = False  #set this to true if working on Colab\n",
    "working_DIR=\"carb_em_project\"\n",
    "working_DIR =  os.path.join(os.getcwd(), working_DIR) \n",
    "\n",
    "\n",
    "if on_Colab:\n",
    "    from google.colab import drive\n",
    "    working_DIR=\"/content/drive/MyDrive/carb_em_project\"\n",
    "\n",
    "weight_table_PATH = working_DIR\n",
    "REGION = 'IT'\n",
    "ENV_FACT='GHG emissions (GWP100) | Problem oriented approach: baseline (CML, 2001) | GWP100 (IPCC, 2007)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1733758101597,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "rtFQsXV9ClTC"
   },
   "outputs": [],
   "source": [
    "#Data cleaning process for coicop codes (hbs_conversion_weight_table)\n",
    "def clean_up(text):\n",
    "    cleaned = text.translate(str.maketrans('', '', string.punctuation+string.digits+'\\n')).lower().strip()\n",
    "    return cleaned\n",
    "\n",
    "def clean_coicop_label(label):\n",
    "    return re.sub(r'\\(.*?\\)', '', label).strip()\n",
    "\n",
    "# Function to process COICOP codes (make them HBS-COMPATIBLE)\n",
    "def process_code(code):\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    code = code.lstrip('c')\n",
    "    code = code.replace('.', '')\n",
    "    return f'EUR_HE{code}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1733758106040,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "VLdviAzQxViV"
   },
   "outputs": [],
   "source": [
    "#path are structured in this decomposed style as they needs to be made dynamic (based on years, countries vars)\n",
    "year=\"2010\"\n",
    "country_ticker=\"IT\"\n",
    "working_DIR=working_DIR+'/'+year\n",
    "if not os.path.exists(working_DIR):\n",
    "    os.makedirs(working_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5096,
     "status": "error",
     "timestamp": 1733758086663,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "L1GzAcze90vh",
    "outputId": "2d0d2c39-8664-4492-95a8-1391a99e4cba"
   },
   "outputs": [],
   "source": [
    "exiobase_Par=working_DIR+\"/IOT_\"+year+\"_pxp.zip\"\n",
    "exio_download = pymrio.download_exiobase3(storage_folder=working_DIR, system=\"pxp\", years=year)\n",
    "exiobase3 = pymrio.parse_exiobase3(path=exiobase_Par)\n",
    "exiobase3.calc_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1733758086668,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "OoK46wcs0p_w"
   },
   "outputs": [],
   "source": [
    "regions = exiobase3.Y.columns.get_level_values(0).unique()\n",
    "Y_a = pd.DataFrame(index=exiobase3.Y.index)\n",
    "for region in regions:\n",
    "    # Select all 7 columns for the region and aggregate them\n",
    "    region_columns = exiobase3.Y.loc[:, region]\n",
    "    Y_a[region] = region_columns.sum(axis=1)\n",
    "\n",
    "P=exiobase3.impacts.S.loc[ENV_FACT]\n",
    "P = P/1000000\n",
    "X = exiobase3.L.dot(Y_a)\n",
    "X = X.multiply(P, axis=0)  # element-wise multiplication\n",
    "display(X)\n",
    "X.to_csv(working_DIR+\"/X.csv\", index=True,  header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pViiuTBs3Us2"
   },
   "source": [
    "### After Exiobase computation, use weight table to compute hbs env factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1222,
     "status": "ok",
     "timestamp": 1733758110306,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "AiUbrNfGC58p",
    "outputId": "eb2b9528-f8fc-480c-fbe8-1089786298f3"
   },
   "outputs": [],
   "source": [
    "#load X () (to avoid Colab disconnection)\n",
    "X = pd.read_csv(working_DIR+\"/X.csv\",  index_col=[0, 1],)\n",
    "regions = X.columns.unique()\n",
    "\n",
    "#load the coicop - exiobase conversion weights map (aka weight_table)\n",
    "weights = pd.read_excel(weight_table_PATH+'/COICOP_EU_ini.xlsx' ,sheet_name='bridge_ini', header=1)\n",
    "weights = weights.iloc[:63, 2:] #exclude false row/cols\n",
    "weights = weights.rename(columns={'Unnamed: 2': 'COICOP'})\n",
    "display(weights)\n",
    "#we match coicop labels from weight_table with hbs codes\n",
    "hash_labels = pd.read_excel(weight_table_PATH +'/COICOP_EXIOBASE.xlsx' ,sheet_name='COICOP_to_EXIOBASEprod', header=None)\n",
    "display(hash_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1733756521735,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "gvvbNDudJI_K",
    "outputId": "a374cda9-fe8b-4e98-b8fb-04cb317bca78"
   },
   "outputs": [],
   "source": [
    "# clean the coicop labels and produce a map that reports the actual HBS code for the given COICOP category\n",
    "hash_labels[1] = hash_labels[1].apply(process_code)\n",
    "hash_labels[2] = hash_labels[2].apply(clean_coicop_label)\n",
    "hash_labels[2] = hash_labels[2].apply(clean_up)\n",
    "hash_map = hash_labels[[1, 2]].set_index(2).to_dict()[1]\n",
    "display(hash_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "gTMhn6uIJN6L"
   },
   "outputs": [],
   "source": [
    "#Manually add missing labels - hbs code (Following HBS DOCUMENTATION)\n",
    "hash_map['nonalcoholic beverages']='EUR_HE012'\n",
    "hash_map['alcoholic beverages']='EUR_HE021'\n",
    "hash_map['clothing']='EUR_HE031'\n",
    "hash_map['footwear']='EUR_HE032'\n",
    "hash_map['actual rent']='EUR_HE041'\n",
    "hash_map['imputed rent']='EUR_HE042'\n",
    "hash_map['maintenance and repair of the dwelling']='EUR_HE043'\n",
    "hash_map['water supply and miscellaneous services reacting to the dwelling']='EUR_HE044'\n",
    "hash_map['household appliances']='EUR_HE053'\n",
    "hash_map['tools and equipment for house and garden']='EUR_HE055'\n",
    "hash_map['medical products appliances and equipment']='EUR_HE061'\n",
    "hash_map['outpatient services'] = 'EUR_HE062'\n",
    "hash_map['purchase of vehicles']='EUR_HE071'\n",
    "hash_map['communication'] = 'EUR_HE08'\n",
    "hash_map['audiovisual photographic and information processing equipment'] = 'EUR_HE091'\n",
    "hash_map['other major durables for recreation and culture']='EUR_HE092'\n",
    "hash_map['other recreational items and equipment gardens and pets'] = 'EUR_HE093'\n",
    "hash_map['recreational and cultural services']='EUR_HE094'\n",
    "hash_map['newspapers books and stationery'] = 'EUR_HE095'\n",
    "hash_map['preprimary and primary education secondary education postsecondary education tertiary education and education not defined by level'] = 'EUR_HE10'\n",
    "hash_map['catering services'] = 'EUR_HE111'\n",
    "hash_map['personal care']='EUR_HE121'\n",
    "hash_map['insurance']='EUR_HE125'\n",
    "hash_map = {key: value.strip() for key, value in hash_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1733756577553,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "ES4-8lBuJOo8",
    "outputId": "b9d87d0d-6380-498b-ea39-10a6a50472d1"
   },
   "outputs": [],
   "source": [
    "#replace coicop labels in original weight_table with actual HBS code using the previously created hash map\n",
    "weights['COICOP'] = weights['COICOP'].apply(clean_up)\n",
    "weights['COICOP'] = weights['COICOP'].replace(hash_map)\n",
    "weights.set_index('COICOP', inplace=True)\n",
    "weights = weights.iloc[:, :-1] #exclude last column (total)\n",
    "display(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean Exiobase labels for both  X and weigth_table\n",
    "X.index = X.index.set_levels(\n",
    "    X.index.levels[1].map(clean_up),  \n",
    "    level=1                   \n",
    ")\n",
    "weights.columns = weights.columns.map(clean_up)\n",
    "display(X)\n",
    "display(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "0c2fk4sGJD4k"
   },
   "outputs": [],
   "source": [
    "# (?maybe create a class so that it always has access to weight table map?, also region as another universal attribute)\n",
    "def get_region_hbs_fts(X_i, region, weight_map):\n",
    "  \"\"\"Given a region X_i (a column from X) it computes the corresponding region footprints for hbs EUR_HExx only . \n",
    "\n",
    "    returns a table in the form: category| footprint|\n",
    "                                 EUR_HExx| value\n",
    "  \"\"\"\n",
    "  data = pd.DataFrame(columns=['category','footprint'])\n",
    "\n",
    "  #Determine HBS Categories env impacts starting from EXIOBASE env impacts  (following weight_map)\n",
    "  regions = X_i.index.levels[0]\n",
    "  categories = weight_map.index\n",
    "\n",
    "  for hbscat in categories:\n",
    "    cat_footprint=0\n",
    "    resp_cat = weights.loc[hbscat]\n",
    "    resp_cat = resp_cat[resp_cat!=0].index.tolist()\n",
    "    for exiocat in resp_cat:\n",
    "      rows_sum=X_i.loc[X_i.index.get_level_values('sector') == exiocat].sum()\n",
    "      cat_footprint += (weights.at[hbscat, exiocat]*rows_sum)\n",
    "\n",
    "    new_row = pd.DataFrame([[hbscat, cat_footprint]], columns=['category', 'footprint'])\n",
    "    data = pd.concat([data, new_row], ignore_index=True)\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 169048,
     "status": "ok",
     "timestamp": 1733754728108,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "5ffn1CBJ9_eV",
    "outputId": "a44147ee-e059-4fed-bdfb-08772384ece0"
   },
   "outputs": [],
   "source": [
    "hbs_hh= pd.read_excel(working_DIR+\"/\"+country_ticker+\"_HBS_hh.xlsx\")\n",
    "display(hbs_hh)\n",
    "if not os.path.exists(working_DIR+\"/out\"):\n",
    "    os.makedirs(working_DIR+\"/out\")\n",
    "\n",
    "#building the footprint tables by adding only Expenditure categories vars of hbs file\n",
    "EUR_HE_cats = [category for category in hbs_hh.columns if re.match(r\"^EUR_HE.*\", category)]  #HOME expens\n",
    "EUR_HJ_cats = [category for category in hbs_hh.columns if re.match(r\"^EUR_HJ.*\", category)]  #ABROAD expens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "error",
     "timestamp": 1733754488428,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "IyviPc9tI58m",
    "outputId": "e340e954-d8f7-4613-d38d-fce452dda53f"
   },
   "outputs": [],
   "source": [
    "#compute for each region hbs EUR_HE env footprints, and add them to ft table\n",
    "footprints =  pd.DataFrame(columns=['region', 'category','footprint'])\n",
    "for region in regions:\n",
    "    o=get_region_hbs_fts(X[region], region,weights)\n",
    "    for index, row in o.iterrows():\n",
    "      new_row = {'region': region, 'category': row['category'], 'footprint': row['footprint']}\n",
    "      footprints.loc[len(footprints)] = new_row\n",
    "\n",
    "display(footprints)\n",
    "footprints.to_csv(working_DIR+\"/footprints.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rag_Fa-V98R4"
   },
   "source": [
    "###Load HBS file structure and Compute Environmental HBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1733758103420,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "3AVEcC4l-Ix6"
   },
   "outputs": [],
   "source": [
    "#HBS Env factors\n",
    "def get_hbs_env(country_hbs,country_env):\n",
    "  \"\"\"\n",
    "  Returns the hbs file where only category expenditures are replaced with corresponding footprint values\n",
    "\n",
    "  country_hbs:\n",
    "              is the hbs_hh file for the given country (and given year) e.g. IT_HBS_2010\n",
    "  country_env:\n",
    "              is a footprint hashmap: where keys are strings of  hbs EUR categories, and values contain unitary footprint associated to the category. (these footprints are specific to the given country and year)\n",
    "              e.g. 'EUR_HE011' = 2\n",
    "                   'EUR_HJ01' =  3\n",
    "                   for IT 2010\n",
    "\n",
    "  NB:The country_env MUST contain all hbs expenditure categories for which env factors are computed, only variables different from such expenditure categories should be omitted (please refer to HBS UserManual)\n",
    "     omitted cat will not be affected.\n",
    "     if the footprint estimate for any expenditure category is not available then its value in country_env must be at 0\n",
    "  \"\"\"\n",
    "\n",
    "  #create extended footprints vector (country specific) with same format as hbs file headers\n",
    "  ex_ft = country_hbs.iloc[0:1].copy() # Create a copy of the original DataFrame\n",
    "  ex_ft.loc[:] = 1\n",
    "\n",
    "\n",
    "  #for each hbs EUR cat in country_env set the value in ex_ft\n",
    "  for EUR_cat in country_env.keys():\n",
    "    if EUR_cat in ex_ft.columns: #safety check, avoids adding non-existing cols\n",
    "      ex_ft[EUR_cat] = country_env[EUR_cat]\n",
    "      country_hbs[EUR_cat]=country_hbs[EUR_cat].fillna(0) #avoids NaN values in later operations\n",
    "\n",
    "\n",
    "  #perform element wise broadcasted multiplication\n",
    "  headers = country_hbs.columns\n",
    "  A = np.array(country_hbs)\n",
    "  x = np.array(ex_ft)\n",
    "\n",
    "  for EUR_cat in country_env.keys():\n",
    "      headers = [header.lower() if header == EUR_cat else header for header in headers]\n",
    "\n",
    "  # Reshape x to make it compatible for broadcasting along the columns of A\n",
    "  # Perform element-wise multiplication using broadcasting\n",
    "  B = A * x  # Each column of A is multiplied by the corresponding element in x\n",
    "  #re-attach hbs headers\n",
    "  B_df = pd.DataFrame(B, columns=headers)\n",
    "  return B_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1732135555712,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "BVacaUv7E0GD",
    "outputId": "d58b448b-f746-4e7a-99ee-61d5eee8fcdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: {'EUR_HE09511', 'EUR_HE03123', 'EUR_HE01213', 'EUR_HE01169', 'EUR_HE06232', 'EUR_HE12111', 'EUR_HE09521', 'EUR_HE09121', 'EUR_HE09222', 'EUR_HE01115', 'EUR_HE02122', 'EUR_HE08111', 'EUR_HE01184', 'EUR_HE07211', 'EUR_HE01164', 'EUR_HE12531', 'EUR_HE01176', 'EUR_HE01134', 'EUR_HE09531', 'EUR_HE03141', 'EUR_HE03211', 'EUR_HE01192', 'EUR_HE01222', 'EUR_HE01211', 'EUR_HE12121', 'EUR_HE01168', 'EUR_HE04211', 'EUR_HE01175', 'EUR_HE06221', 'EUR_HE01221', 'EUR_HE01172', 'EUR_HE11112', 'EUR_HE01124', 'EUR_HE01127', 'EUR_HE01173', 'EUR_HE11211', 'EUR_HE12521', 'EUR_HE01133', 'EUR_HE07341', 'EUR_HE09311', 'EUR_HE01111', 'EUR_HE04541', 'EUR_HE01143', 'EUR_HE09341', 'EUR_HE07361', 'EUR_HE01141', 'EUR_HE09141', 'EUR_HE01191', 'EUR_HE01146', 'EUR_HE01116', 'EUR_HE10411', 'EUR_HE12412', 'EUR_HE03212', 'EUR_HE01167', 'EUR_HE03111', 'EUR_HE06121', 'EUR_HE05316', 'EUR_HE01144', 'EUR_HE01114', 'EUR_HE06131', 'EUR_HE07131', 'EUR_HE01181', 'EUR_HE12321', 'EUR_HE05311', 'EUR_HE05313', 'EUR_HE05314', 'EUR_HE03131', 'EUR_HE12411', 'EUR_HE02111', 'EUR_HE01183', 'EUR_HE09351', 'EUR_HE05121', 'EUR_HE01162', 'EUR_HE04121', 'EUR_HE04222', 'EUR_HE09422', 'EUR_HE01112', 'EUR_HE12551', 'EUR_HE04421', 'EUR_HE01147', 'EUR_HE04522', 'EUR_HE07111', 'EUR_HE05321', 'EUR_HE05521', 'EUR_HE02212', 'EUR_HE05331', 'EUR_HE01153', 'EUR_HE05621', 'EUR_HE11111', 'EUR_HE01186', 'EUR_HE01165', 'EUR_HE05211', 'EUR_HE03213', 'EUR_HE08311', 'EUR_HE10211', 'EUR_HE07141', 'EUR_HE01171', 'EUR_HE09541', 'EUR_HE02213', 'EUR_HE09131', 'EUR_HE01174', 'EUR_HE04411', 'EUR_HE05312', 'EUR_HE07112', 'EUR_HE01193', 'EUR_HE09231', 'EUR_HE04321', 'EUR_HE07231', 'EUR_HE05622', 'EUR_HE04111', 'EUR_HE09421', 'EUR_HE04531', 'EUR_HE12322', 'EUR_HE01152', 'EUR_HE01145', 'EUR_HE01177', 'EUR_HE07121', 'EUR_HE09111', 'EUR_HE09122', 'EUR_HE01161', 'EUR_HE03221', 'EUR_HE07221', 'EUR_HE07311', 'EUR_HE01166', 'EUR_HE10311', 'EUR_HE09331', 'EUR_HE05611', 'EUR_HE07351', 'EUR_HE10111', 'EUR_HE01224', 'EUR_HE01131', 'EUR_HE01142', 'EUR_HE07331', 'EUR_HE04511', 'EUR_HE01178', 'EUR_HE07321', 'EUR_HE09221', 'EUR_HE09151', 'EUR_HE09112', 'EUR_HE03122', 'EUR_HE12131', 'EUR_HE05612', 'EUR_HE01155', 'EUR_HE01125', 'EUR_HE02211', 'EUR_HE02131', 'EUR_HE09321', 'EUR_HE01151', 'EUR_HE01163', 'EUR_HE03121', 'EUR_HE01223', 'EUR_HE12621', 'EUR_HE01123', 'EUR_HE04551', 'EUR_HE05131', 'EUR_HE09424', 'EUR_HE06233', 'EUR_HE09423', 'EUR_HE11121', 'EUR_HE09211', 'EUR_HE09431', 'EUR_HE04311', 'EUR_HE01194', 'EUR_HE01185', 'EUR_HE07241', 'EUR_HE01132', 'EUR_HE12311', 'EUR_HE09411', 'EUR_HE06211', 'EUR_HE04431', 'EUR_HE04221', 'EUR_HE01113', 'EUR_HE12541', 'EUR_HE05511', 'EUR_HE01121', 'EUR_HE01182', 'EUR_HE06231', 'EUR_HE05412', 'EUR_HE05315', 'EUR_HE05317', 'EUR_HE02121', 'EUR_HE06111', 'EUR_HE01126', 'EUR_HE12211', 'EUR_HE01154', 'EUR_HE08211', 'EUR_HE04521', 'EUR_HE01122', 'EUR_HE05111', 'EUR_HE05411', 'EUR_HE01212', 'EUR_HE04441', 'EUR_HE05413', 'EUR_HE05414', 'EUR_HE10511'}, 3: {'EUR_HE0733', 'EUR_HE0212', 'EUR_HE0612', 'EUR_HE0621', 'EUR_HE0934', 'EUR_HE1262', 'EUR_HE0115', 'EUR_HE1211', 'EUR_HE0922', 'EUR_HE1232', 'EUR_HE1021', 'EUR_HE0512', 'EUR_HE0322', 'EUR_HE1231', 'EUR_HE0711', 'EUR_HE0713', 'EUR_HE0533', 'EUR_HE0453', 'EUR_HE1221', 'EUR_HE0935', 'EUR_HE0114', 'EUR_HE0452', 'EUR_HE0613', 'EUR_HE0454', 'EUR_HE0431', 'EUR_HE1111', 'EUR_HE0941', 'EUR_HE0923', 'EUR_HE0113', 'EUR_HE0921', 'EUR_HE0943', 'EUR_HE0724', 'EUR_HE0531', 'EUR_HE1255', 'EUR_HE0513', 'EUR_HE0312', 'EUR_HE0731', 'EUR_HE0562', 'EUR_HE0954', 'EUR_HE0116', 'EUR_HE0221', 'EUR_HE0714', 'EUR_HE0541', 'EUR_HE0451', 'EUR_HE0811', 'EUR_HE0122', 'EUR_HE0111', 'EUR_HE0722', 'EUR_HE0313', 'EUR_HE1031', 'EUR_HE0213', 'EUR_HE0735', 'EUR_HE0914', 'EUR_HE0911', 'EUR_HE0444', 'EUR_HE0411', 'EUR_HE0455', 'EUR_HE0561', 'EUR_HE0712', 'EUR_HE0732', 'EUR_HE0443', 'EUR_HE1271', 'EUR_HE0421', 'EUR_HE0442', 'EUR_HE1051', 'EUR_HE0551', 'EUR_HE0931', 'EUR_HE0952', 'EUR_HE1121', 'EUR_HE0311', 'EUR_HE0321', 'EUR_HE0118', 'EUR_HE0723', 'EUR_HE0933', 'EUR_HE0915', 'EUR_HE0951', 'EUR_HE1213', 'EUR_HE0117', 'EUR_HE0211', 'EUR_HE0521', 'EUR_HE0912', 'EUR_HE0112', 'EUR_HE0422', 'EUR_HE0611', 'EUR_HE1011', 'EUR_HE0622', 'EUR_HE0532', 'EUR_HE0821', 'EUR_HE0721', 'EUR_HE0119', 'EUR_HE1041', 'EUR_HE0552', 'EUR_HE1212', 'EUR_HE1241', 'EUR_HE1112', 'EUR_HE0441', 'EUR_HE0942', 'EUR_HE0932', 'EUR_HE0734', 'EUR_HE0412', 'EUR_HE1252', 'EUR_HE1254', 'EUR_HE0913', 'EUR_HE0953', 'EUR_HE0121', 'EUR_HE1253', 'EUR_HE0314', 'EUR_HE0736', 'EUR_HE0511', 'EUR_HE0231', 'EUR_HE0831', 'EUR_HE0432', 'EUR_HE0623'}, 2: {'EUR_HE023', 'EUR_HE021', 'EUR_HE091', 'EUR_HE105', 'EUR_HE022', 'EUR_HE051', 'EUR_HE062', 'EUR_HE056', 'EUR_HE102', 'EUR_HE071', 'EUR_HE054', 'EUR_HE082', 'EUR_HE123', 'EUR_HE126', 'EUR_HE104', 'EUR_HE093', 'EUR_HE124', 'EUR_HE061', 'EUR_HE112', 'EUR_HE081', 'EUR_HE101', 'EUR_HE073', 'EUR_HE012', 'EUR_HE125', 'EUR_HE121', 'EUR_HE092', 'EUR_HE055', 'EUR_HE096', 'EUR_HE045', 'EUR_HE072', 'EUR_HE127', 'EUR_HE053', 'EUR_HE122', 'EUR_HE043', 'EUR_HE052', 'EUR_HE111', 'EUR_HE042', 'EUR_HE094', 'EUR_HE031', 'EUR_HE011', 'EUR_HE044', 'EUR_HE083', 'EUR_HE095', 'EUR_HE063', 'EUR_HE032', 'EUR_HE103', 'EUR_HE041'}, 1: {'EUR_HE11', 'EUR_HE02', 'EUR_HE10', 'EUR_HE06', 'EUR_HE08', 'EUR_HE01', 'EUR_HE09', 'EUR_HE12', 'EUR_HE04', 'EUR_HE07', 'EUR_HE00', 'EUR_HE03', 'EUR_HE05'}}\n"
     ]
    }
   ],
   "source": [
    "#HBS structure presents EUR_HE cat in 4 levels (where first means the top-level categories):\n",
    "level_map = {}\n",
    "for i in range(5, 1, -1):\n",
    "  #determine level (i-1) EUR categories using RegEx and HBS structure guidelines\n",
    "  pattern = rf\"EUR_HE\\d{{{i}}}\\b\"\n",
    "  curr_level_cat = [category for category in (set(EUR_HE_cats)) if re.match(pattern, category)]\n",
    "  curr_level_cat = set(curr_level_cat)\n",
    "  level_map[i-1] = curr_level_cat\n",
    "\n",
    "print(level_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "kpWWcN5XKA_x"
   },
   "outputs": [],
   "source": [
    "class Aggregator():\n",
    "\n",
    "    def __init__(self, level_map):\n",
    "        \"\"\"\n",
    "        A simple aggregator\n",
    "        takes an hash map, where key is the level and value is a set of labels categories for the level\n",
    "        where each element is a set of labels categories for the specific level\n",
    "        lev_labels_list first element is a set of categories for the first level, ..., lev_labels_list[i] must contain set categories for level[i]\n",
    "\n",
    "\n",
    "        the order is dictated by the fact that categories at level [i+1] are subcategories of categories at level[i]\n",
    "        \"\"\"\n",
    "\n",
    "        self.number_levels = len(level_map)\n",
    "        self.level_map = level_map\n",
    "\n",
    "\n",
    "    def get_aggregates(self, df):\n",
    "        \"\"\"Pass a dataframe that contains as column headers the set given by the union of label sets present in each level of the level_map\n",
    "          the values of such df are the ones that needs to be aggregated column-wise\n",
    "\n",
    "        \"\"\"\n",
    "        aggregates = df.copy()\n",
    "        #from the bottom level to upper level (0) (it skips the lowest level as no aggregation is needed)\n",
    "        for level in range(len(self.level_map)-1, 0,-1):\n",
    "            current_lev_cats = self.level_map[level]\n",
    "            for cat in current_lev_cats:\n",
    "                subcat_to_sum = self.level_map[level+1]\n",
    "                pattern = rf\"^{re.escape(cat)}\"\n",
    "                subcat_to_sum = set([eur_cat for eur_cat in subcat_to_sum if re.match(pattern, eur_cat)])\n",
    "                #aggregate level-wise (bottom-up) for EUR categories\n",
    "                #recall now modified cat are in lower_case\n",
    "\n",
    "                Sum = pd.DataFrame({\n",
    "                    'sum': [0] * len(df)\n",
    "                })\n",
    "\n",
    "                for subcat in subcat_to_sum:\n",
    "                  Sum['sum'] +=  df[subcat.lower()]\n",
    "\n",
    "                if (aggregates[cat.lower()] == 0).all():\n",
    "                    aggregates[cat.lower()] = Sum['sum']\n",
    "                elif (aggregates[cat.lower()] < Sum['sum']).any():\n",
    "                    print('Please check correctness of your footprint estimates! for '+cat+' category and '+str(subcat_to_sum))\n",
    "\n",
    "\n",
    "        return aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1733754767897,
     "user": {
      "displayName": "Michele",
      "userId": "17210905712930101532"
     },
     "user_tz": -60
    },
    "id": "PP846oMA-S6_",
    "outputId": "660c3371-1c7b-4e6d-bb68-0d18221ad452"
   },
   "outputs": [],
   "source": [
    "ft = dict(zip(EUR_HE_cats, [0]*len(EUR_HE_cats)))   #from here one can notice that an envfactor of zero for a given expcategory  does not necessarily mean that its associated footprint is actually 0 (can be that we have no data on footprint of the category)\n",
    "#ft will later contain the effective  region specific footprints for each category\n",
    "aggregator = Aggregator(level_map)\n",
    "\n",
    "regions=[\"IT\"]#do the following per each region:\n",
    "for region in regions:\n",
    "  footprints = pd.read_csv(working_DIR+\"/footprints.csv\")\n",
    "  filtered_fact = footprints[footprints['region'] == REGION]\n",
    "\n",
    "  for index, row in filtered_fact.iterrows():\n",
    "      if row['category'] in ft.keys(): #avoids adding non-existing EUR_cat to ft\n",
    "        ft[row['category']] = row['footprint']\n",
    "\n",
    "  result = get_hbs_env(hbs_hh,ft)\n",
    "  #result.to_csv(working_DIR+\"/out/\"+country_ticker+'_'+year+'_'+\"ENVHBS_hh.csv\", index=False)\n",
    "  #display(result)\n",
    "\n",
    "  #aggregate\n",
    "  result = aggregator.get_aggregates(result)\n",
    "\n",
    "  result.to_csv(working_DIR+\"/out/\"+country_ticker+'_'+year+'_'+\"ENVHBS_hh.csv\", index=False)\n",
    "\n",
    "\n",
    "  #compute EUR_HJ counterpart\n",
    "  ft=dict(zip(EUR_HJ_cats, [0]*len(EUR_HJ_cats)))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjtvrdoCKSLv"
   },
   "outputs": [],
   "source": [
    "\n",
    "for region in regions:\n",
    "     #inglobe them into some aggregator class\n",
    "    #aggregate level-wise (bottom-up) for EUR_HJ categories (only eur_hj00, but follows different convention)\n",
    "    #recall now modified cat are in lower_case\n",
    "    saving_DIR = working_DIR+\"/out/\"+region+'_'+year+'_'+\"agg_ENVHBS_hh.csv\"\n",
    "    result =  pd.read_csv(saving_DIR)\n",
    "    \n",
    "    #eur_he00 follows special aggregation process\n",
    "    Sum = pd.DataFrame({\n",
    "        'sum': [0] * len(result)\n",
    "    })\n",
    "    level_cats = level_map[1] - set('EUR_HE00')\n",
    "    for cat in level_cats:\n",
    "        Sum['sum'] +=  result[cat.lower()]\n",
    "\n",
    "        if (result['eur_he00'] == 0).all():\n",
    "            result['eur_he00'] = Sum['sum']\n",
    "        elif (result['eur_he00'] < Sum['sum']).any():\n",
    "            print('Please check correctness of your footprint estimates! for EUR_HJ00 category and '+str(level_cats))\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    Sum = pd.DataFrame({\n",
    "        'sum': [0] * len(result)\n",
    "    })\n",
    "    level_cats = set(EUR_HJ_cats) - set(['EUR_HJ00', 'EUR_HJ90'])\n",
    "    for cat in level_cats:\n",
    "        Sum['sum'] +=  result[cat.lower()]\n",
    "\n",
    "        if (result['eur_hj00'] == 0).all():\n",
    "            result['eur_hj00'] = Sum['sum']\n",
    "        elif (result['eur_hj00'] < Sum['sum']).any():\n",
    "            print('Please check correctness of your footprint estimates! for EUR_HJ00 category and '+str(level_cats))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    result.to_csv(saving_DIR, index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hash_map for manual matching indirect home (HJ_xx) exploiting HE_xx\n",
    "i_hash_map = {}\n",
    "\n",
    "#Manually add missing labels - hbs code (Following HBS DOCUMENTATION)\n",
    "i_hash_map['EUR_HJ00'] = ''\n",
    "i_hash_map['EUR_HJ01']='EUR_HE01' #Food and non-alcoholic beverages\n",
    "i_hash_map['EUR_HJ02']='EUR_HE02' #Alcoholic beverages, tobacco and narcotics\n",
    "i_hash_map['EUR_HJ03']='' #Clothing and footwear\n",
    "i_hash_map['EUR_HJ04']='' #Housing, water, electricity, gas and other fuels\n",
    "i_hash_map['EUR_HJ05']='' #Furnishings, household equipment and routine household maintenance\n",
    "i_hash_map['EUR_HJ06']='' #Health\n",
    "i_hash_map['EUR_HJ07']='' #Transport\n",
    "i_hash_map['EUR_HJ08']='' #Communication\n",
    "i_hash_map['EUR_HJ09']='' #Recreation and culture\n",
    "i_hash_map['EUR_HJ10']='' #Education\n",
    "i_hash_map['EUR_HJ11']='' #Restaurants and hotels\n",
    "i_hash_map['EUR_HJ12']='' #Miscellaneous goods and services\n",
    "\n",
    "#instead of doing it manually\n",
    "for key in i_hash_map:\n",
    "    i_hash_map[key] = key.replace('J', 'E')\n",
    "\n",
    "i_hash_map['EUR_HJ90'] = '' #Consumption expenditure on travelling and holidays done abroad  (no MATCH for EUR_HExx)\n",
    "\n",
    "\n",
    "def get_region_hbs_hj_fts(region, hjcat, ft_table):\n",
    "    \"\"\"computes the hjcat for the region, using footprint_table that will contain footprint of EUR_HE categories for all regions\n",
    "\n",
    "    Args:\n",
    "        region (_type_): _description_\n",
    "        hjcat (_type_): _description_\n",
    "        footprint_table (_type_): _description_\n",
    "    \"\"\"\n",
    "    return ft_table \n",
    "\n",
    "#AFTER computing hbs for EUR_HE use aggregated values and i_hashmap to estimate env fact for EUR_HJ using get_region_hbs_hj_fts\n",
    "#then multiply such values for values in hbs\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMhsazwJ2NtRc0z/Zdsf+xJ",
   "mount_file_id": "11oKnHHLswjXyMLWgTeG1msFFTjxcWYiq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
